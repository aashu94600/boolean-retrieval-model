{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter the number of raw documents in the folder local_corpus10\n"
     ]
    }
   ],
   "source": [
    "number_of_doucuments = 10 #default value\n",
    "number_of_doucuments = int(raw_input('enter the number of raw documents in the folder local_corpus'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#read data from corpus\n",
    "doc_collection = []\n",
    "for i in range(number_of_doucuments) :\n",
    "    f = open('local_corpus/doc'+str(i+1)+'.txt')\n",
    "    doc_collection.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_vocabulary(doc_collection) :\n",
    "    s = []\n",
    "    for i in range(len(doc_collection)) :\n",
    "        l = doc_collection[i].split(' ')\n",
    "        for j in range(len(l)) :\n",
    "            s.append(l[j])\n",
    "    myset = set(s)\n",
    "    vocabulary = list(myset)\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '\\n', ',', '.', '.\\n', '200', '250', 'a', 'accounting', 'activity', 'algorithms', 'all', 'american', 'an', 'analysis', 'and', 'annual', 'appears', 'applying', 'are', 'art', 'artificial', 'association', 'audience', 'available', 'ball', 'being', 'boosting', 'bowl', 'broadcasts', 'by', 'classified', 'clustering', 'collection', 'component', 'computers', 'context', 'decision', 'decisions', 'decomposition', 'degrees', 'depending', 'designed', 'eight', 'engine', 'etc', 'explicitly', 'family', 'feedback', 'football', 'for', 'forests', 'form', 'from', 'game', 'globally', 'goal', 'group', 'has', 'highest', 'history', 'in', 'independent', 'information', 'into', 'involve', 'is', 'it', 'kicking', 'learning', 'linear', 'logistic', 'machine', 'making', 'members', 'million', 'most', 'nations', 'nature', 'need', 'network', 'neural', 'obtaining', 'of', 'on', 'or', 'over', 'played', 'players', 'politics', 'popular', 'predictors', 'principal', 'process', 'programmed', 'random', 'refer', 'regional', 'regression', 'reinforcement', 'relevant', 'resources', 'retrieval', 'score', 'search', 'seven', 'signal', 'singular', 'software', 'sport', 'sports', 'states', 'super', 'supervised', 'system', 'tasks', 'team', 'television', 'that', 'the', 'to', 'top', 'training', 'trees', 'typically', 'u.s.', 'understood', 'uniform', 'united', 'unqualified', 'unsupervised', 'value', 'various', 'varying', 'watched', 'web', 'which', 'whichever', 'wide', 'with', 'without', 'word', 'world']\n"
     ]
    }
   ],
   "source": [
    "# convert words into lowercase\n",
    "lexicon = get_vocabulary(doc_collection)\n",
    "for i in range(len(lexicon)) :\n",
    "    lexicon[i] = lexicon[i].lower()\n",
    "myset = set(lexicon)\n",
    "lexicon = list(myset)\n",
    "lexicon.sort()\n",
    "print lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['200', '250', 'a', 'accounting', 'activity', 'algorithms', 'all', 'american', 'an', 'analysis', 'and', 'annual', 'appears', 'applying', 'are', 'art', 'artificial', 'association', 'audience', 'available', 'ball', 'being', 'boosting', 'bowl', 'broadcasts', 'by', 'classified', 'clustering', 'collection', 'component', 'computers', 'context', 'decision', 'decisions', 'decomposition', 'degrees', 'depending', 'designed', 'eight', 'engine', 'etc', 'explicitly', 'family', 'feedback', 'football', 'for', 'forests', 'form', 'from', 'game', 'globally', 'goal', 'group', 'has', 'highest', 'history', 'in', 'independent', 'information', 'into', 'involve', 'is', 'it', 'kicking', 'learning', 'linear', 'logistic', 'machine', 'making', 'members', 'million', 'most', 'nations', 'nature', 'need', 'network', 'neural', 'obtaining', 'of', 'on', 'or', 'over', 'played', 'players', 'politics', 'popular', 'predictors', 'principal', 'process', 'programmed', 'random', 'refer', 'regional', 'regression', 'reinforcement', 'relevant', 'resources', 'retrieval', 'score', 'search', 'seven', 'signal', 'singular', 'software', 'sport', 'sports', 'states', 'super', 'supervised', 'system', 'tasks', 'team', 'television', 'that', 'the', 'to', 'top', 'training', 'trees', 'typically', 'u.s.', 'understood', 'uniform', 'united', 'unqualified', 'unsupervised', 'value', 'various', 'varying', 'watched', 'web', 'which', 'whichever', 'wide', 'with', 'without', 'word', 'world']\n"
     ]
    }
   ],
   "source": [
    "# remove irrelevant words...the list of irrelevant words can be modified implicitly by seeing the content of the text corpus\n",
    "irrelevant_words = ['', '\\n', ',', '.', '.\\n']\n",
    "temp=[]\n",
    "for word in lexicon:\n",
    "    if word not in irrelevant_words:\n",
    "        temp.append(word)\n",
    "lexicon=temp\n",
    "del temp\n",
    "print lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2], [2], [0, 4, 7, 8, 9], [2], [8], [5, 6], [7], [2], [8], [5], [2, 4], [2], [1], [7], [4, 5, 6], [3], [6], [2], [2], [4], [0], [3], [6], [2], [2], [2], [4], [5], [8], [5], [3], [1], [6], [7], [5], [0], [4], [9], [2], [9], [5, 6], [3], [0], [4], [0, 1, 2], [2, 9], [6], [1], [8], [2], [2], [0], [7], [2], [2], [2], [1, 2, 5, 6], [5], [8, 9], [4], [0], [0, 1, 2, 3, 7, 8, 9], [2], [0], [3, 4, 5, 6], [6], [6], [3, 4, 5, 6], [2, 7], [7], [2], [1, 2], [2], [4], [8], [6], [6], [8], [0, 1, 2, 3, 4, 7, 8], [4, 9], [4], [2], [2], [2], [7], [1, 2], [6], [5], [7], [3], [6], [1], [1], [6], [4], [8], [8], [8], [0], [9], [2], [4], [5], [9], [2], [0], [2], [2], [4, 6], [4, 9], [4], [0], [2], [0, 9], [1, 2, 3, 4, 7, 8, 9], [0, 1, 4, 7, 8, 9], [2], [3], [6], [4], [2], [1], [7], [2], [1], [4, 5], [5], [5, 6], [0], [2], [9], [1], [1], [9], [2], [3], [1], [2, 9]]\n"
     ]
    }
   ],
   "source": [
    "# build inverted index data structure for boolean retrieval system\n",
    "inverted_index = []\n",
    "for i in range(len(lexicon)) :\n",
    "    inverted_index.append([])\n",
    "    for j in range(len(doc_collection)) :\n",
    "        l = doc_collection[j].split(' ')\n",
    "        for k in range(len(l)) :\n",
    "            l[k] = l[k].lower()\n",
    "        if lexicon[i] in l :\n",
    "            inverted_index[i].append(j)\n",
    "print inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 5, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 2, 1, 1, 7, 1, 1, 4, 1, 1, 4, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 7, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 7, 6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "term_frequency = [0 for i in range(len(lexicon))]\n",
    "for i in range(len(lexicon)) :\n",
    "    term_frequency[i] = len(inverted_index[i])\n",
    "print term_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# binary search to find id of word in vocabulary/lexicon\n",
    "def get_word_id(word) :\n",
    "    l,r = 0,len(lexicon)-1\n",
    "    while(l<=r) :\n",
    "        m = (l+r)/2\n",
    "        if lexicon[m]==word : \n",
    "            return m\n",
    "        elif lexicon[m]<word :\n",
    "            l=m+1\n",
    "        else :\n",
    "            r=m-1\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# efficient two pointer approach for the intersection of 2 postings list , as mentioned in the book\n",
    "def intersection(word1 , word2) :\n",
    "    answer=[]\n",
    "    id1 , id2 = get_word_id(word1) , get_word_id(word2)\n",
    "    if id1==-1 or id2==-1 : \n",
    "        return answer\n",
    "    l1 , l2 = inverted_index[id1] , inverted_index[id2]\n",
    "    p1,p2=0,0\n",
    "    while p1!=len(l1) and p2!=len(l2) :\n",
    "        if l1[p1]==l2[p2] : \n",
    "            answer.append(l1[p1])\n",
    "            p1+=1\n",
    "            p2+=1\n",
    "        elif l1[p1]<l2[p2] :\n",
    "            p1+=1\n",
    "        else :\n",
    "            p2+=1\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# efficient approach for the union of 2 postings list\n",
    "def union(word1,word2):\n",
    "    # l : number of documents\n",
    "    l = len(doc_collection)\n",
    "    # f[i] stores how many words out of word1 or word2 appear in document i\n",
    "    f = [0 for i in range(l)]\n",
    "    answer=[]\n",
    "    id1 , id2 = get_word_id(word1) , get_word_id(word2)\n",
    "    if id1==-1 and id2==-1 : return []\n",
    "    if id1==-1 : return inverted_index[id2]\n",
    "    if id2==-1 : return inverted_index[id1]\n",
    "    l1 , l2 = inverted_index[id1] , inverted_index[id2]\n",
    "    for i in l1 : f[i]+=1\n",
    "    for i in l2 : f[i]+=1\n",
    "    for i in range(l) :\n",
    "        if f[i]>0 : answer.append(i)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def negation(word) :\n",
    "    answer=[]\n",
    "    id1 = get_word_id(word)\n",
    "    if id1==-1 : return [i for i in range(len(doc_collection))]\n",
    "    l1 = inverted_index[id1]\n",
    "    # l : number of documents\n",
    "    l = len(doc_collection)\n",
    "    # f[i] stores does word appear in document i\n",
    "    f = [0 for i in range(l)]\n",
    "    for i in l1 : f[i]=1\n",
    "    for i in range(l) : \n",
    "        if f[i]==0 : answer.append(i)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# efficient two pointer approach for the intersection of 2 postings list , as mentioned in the book\n",
    "def intersection_of_postings(l1 , l2) :\n",
    "    answer=[]\n",
    "    p1,p2=0,0\n",
    "    while p1!=len(l1) and p2!=len(l2) :\n",
    "        if l1[p1]==l2[p2] : \n",
    "            answer.append(l1[p1])\n",
    "            p1+=1\n",
    "            p2+=1\n",
    "        elif l1[p1]<l2[p2] :\n",
    "            p1+=1\n",
    "        else :\n",
    "            p2+=1\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# efficient approach for the intersection of n postings list , as mentioned in the book\n",
    "def n_intersection(words_list) :\n",
    "    #id1 stores id of words whose postings are to be intersected\n",
    "    id1,answer=[],[]\n",
    "    for word in words_list : id1.append(get_word_id(word))\n",
    "    tuples_list=[]\n",
    "    for i in range(len(id1)) : \n",
    "        tuples_list.append((term_frequency[id1[i]],id1[i]))\n",
    "    #sort postings list according to term frequency for best order of query processing\n",
    "    tuples_list.sort()\n",
    "    id0 = tuples_list[0][1]\n",
    "    if id0!=-1 : answer = inverted_index[id0]\n",
    "    for i in range(1,len(tuples_list)) :\n",
    "        word_id = tuples_list[i][1]\n",
    "        if word_id == -1 : continue\n",
    "        answer = intersection_of_postings(answer , inverted_index[word_id])\n",
    "    return answer    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# efficient approach for the union of n postings list\n",
    "def n_union(words_list):\n",
    "    # l : number of documents\n",
    "    l = len(doc_collection)\n",
    "    # f[i] stores how many words out of word1 or word2 appear in document i\n",
    "    f = [0 for i in range(l)]\n",
    "    answer=[]\n",
    "    id_list=[]\n",
    "    for word in words_list : id_list.append(get_word_id(word))\n",
    "    for i in range(len(words_list)):\n",
    "        id1 = id_list[i]\n",
    "        if id1==-1: continue\n",
    "        l1 = inverted_index[id1]\n",
    "        for j in l1 : f[j]=1\n",
    "    for i in range(l) :\n",
    "        if f[i]>0 : answer.append(i)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter type of query -> 1,2,3:\n",
      "[1.] AND \n",
      "[2.] OR \n",
      "[3.] NEGATION \n",
      "2\n",
      "Enter number of terms to take union : 2\n",
      "Enter word number  1\n",
      "politics\n",
      "Enter word number  2\n",
      "football\n",
      "The relevant documents are :\n",
      "Football is a family of team sports that involve , to varying degrees , kicking a ball to score a goal .\n",
      "\n",
      "Unqualified , the word football is understood to refer to whichever form of football is the most popular in the regional context in which the word appears .\n",
      "\n",
      "Globally , association football is played by over 250 million players in over 200 nations , and has the highest television audience in sport , making it the most popular in the world , American football is the most popular sport in the United States , with the annual Super Bowl game accounting for seven of the top eight of the most watched broadcasts in U.S. television history .\n",
      "\n",
      "Politics is the process of making uniform decisions applying to all members of a group .\n",
      "\n",
      "Y/y to continue, Q/q to quitq\n"
     ]
    }
   ],
   "source": [
    "#take input queries from user\n",
    "while 1:\n",
    "    print \"Enter type of query -> 1,2,3:\"\n",
    "    print \"[1.] AND \"\n",
    "    print \"[2.] OR \"\n",
    "    print \"[3.] NEGATION \"\n",
    "    type = int(raw_input())\n",
    "    if type==1:\n",
    "        n = int(raw_input('Enter number of terms to take intersection : '))\n",
    "        if n<1:\n",
    "            print \"enter valid inputs only.\"\n",
    "            continue\n",
    "        l=[]\n",
    "        for i in range(n):\n",
    "            print 'Enter word number ',i+1\n",
    "            word= raw_input()\n",
    "            l.append(word)\n",
    "        doc_id = n_intersection(l)\n",
    "        print \"The relevant documents are :\"\n",
    "        for id1 in doc_id :\n",
    "            print doc_collection[id1]\n",
    "    elif type==2:\n",
    "        n = int(raw_input('Enter number of terms to take union : '))\n",
    "        if n<1:\n",
    "            print \"enter valid inputs only.\"\n",
    "            continue\n",
    "        l=[]\n",
    "        for i in range(n):\n",
    "            print 'Enter word number ',i+1\n",
    "            word= raw_input()\n",
    "            l.append(word)\n",
    "        doc_id = n_union(l)\n",
    "        print \"The relevant documents are :\"\n",
    "        for id1 in doc_id :\n",
    "            print doc_collection[id1]\n",
    "    elif type==3:\n",
    "        word = raw_input('Enter the word to take negation : ')\n",
    "        doc_id = negation(word)\n",
    "        print \"The relevant documents are :\"\n",
    "        for id1 in doc_id :\n",
    "            print doc_collection[id1]\n",
    "    else : \n",
    "        print \"enter valid inputs only\"\n",
    "        continue\n",
    "    c = raw_input(\"Y/y to continue, Q/q to quit\")\n",
    "    if c=='q' or c=='Q' : break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
